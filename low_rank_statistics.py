##############################################################################
# Functions to compute Low-Rank statistics
# Authored by Ammar Mian, 10/01/2019
# e-mail: ammar.mian@centralesupelec.fr
##############################################################################
# Copyright 2018 @CentraleSupelec
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
##############################################################################
import numpy as np
import scipy as sp
import warnings
from generic_functions import *




# ----------------------------------------------------------------------------
# 1) Low-Rank Gaussian CM equality statistic
# ----------------------------------------------------------------------------

def Gaussian_Log_Likelihood(ğ—, ğ›, ğšº):
    """ Gaussian Log-Likelihood of i.i.d observations
        ----------------------------------------------
        Inputs:
        --------
            * ğ— = a (p, N) numpy array with:
                * p = dimension of vectors
                * N = number of Samples
            * ğ› = mean (array of dimension p)
            * ğšº = covariance matrix of dimension (p,p)
        Outputs:
        ---------
            * the Log-Likelihood """
    
    (p, N) = ğ—.shape
    ğ—_centered = ğ— - np.tile(ğ›.reshape((p,1)), (1,N))
    ğ’ = SCM(ğ—_centered)
    â„“ogâ„’ = - N*p*np.log(np.pi) - N*np.log(np.abs(np.linalg.det(ğšº))) - \
            np.trace(ğ’@np.linalg.inv(ğœ®))
    return â„“ogâ„’

def LR_ğœ®(ğœ®, R,  Ïƒ2):
    """ Low-Rank operator on ğœ®
        ----------------------------------------------
        Inputs:
        --------
            * ğšº = covariance matrix of dimension (p,p)
            * R = rank
            * Ïƒ2 = noise level
        Outputs:
        ---------
            * the LR regularised matrix """

    (p,p) = ğœ®.shape
    u, s, vh = np.linalg.svd(ğœ®)
    if Ïƒ2 is None:
        Ïƒ2 = np.mean(s[R:])
    s_signal = np.max([s[:R],Ïƒ2*np.ones((R,))], axis=0)
    s_noise = Ïƒ2 * np.ones((p-R,))
    s = np.diag(np.hstack([s_signal, s_noise]))
    ğšº_R = u @ s @ u.conj().T

    return ğšº_R


def Rank_estimation(ğ—):
    """ Not implemented yet but model order selection
        ----------------------------------------------
        Inputs:
        --------
            * ğ— = a (p, N) numpy array with:
                * p = dimension of vectors
                * N = number of Samples
        Outputs:
        ---------
            * the Rank """

    (p, N) = ğ—.shape
    return p 


def Ïƒ2_estimaton(ğ—, R):
    """ Estimate Ïƒ2 locally using samples
        ----------------------------------------------
        Inputs:
        --------
            * ğ— = a (p, N) numpy array with:
                * p = dimension of vectors
                * N = number of Samples
            * R = rank
        Outputs:
        ---------
            * Ïƒ2 """

    (p, N) = ğ—.shape
    ğšº = SCM(ğ—)
    u, s, vh = np.linalg.svd(ğšº)
    if R<p:
        Ïƒ2 = s[R:].mean()
    else:
        Ïƒ2 = 0
    return Ïƒ2


def LR_CM_equality_test(ğ—, args):
    """ Gaussian Low-Rank covariance matrix equality GLRT
        --------------------------------------------------
        Inputs:
        --------
            * ğ— = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples
                * T = Number of groups of samples
            * args = (R, Ïƒ2, scale) with
                * R = rank (put 0 for adaptive estimation)
                * Ïƒ2 = noise level (put 0 for adaptive estimation)
                * scale = 'linear' or 'log'
        Outputs:
        ---------
            * the GLRT """

    # 1) Useful variables
    (p, N, T) = ğ—.shape
    R, Ïƒ2, scale = args

    # 2) Estimate R and Ïƒ2 if needed
    if not R:
        R = Rank_estimation(ğ—.reshape((p, N*T)))
    if not Ïƒ2:
        Ïƒ2 = Ïƒ2_estimaton(ğ—.reshape((p,N*T)), R)

    # 3) Estimate ğšº_R under â„‹0 hypothesis
    ğœ® = SCM(X.reshape((p,N*T)))
    ğœ®_R = LR_ğœ®(ğœ®, R, Ïƒ2)

    # 4) Estimate ğšºt_R under â„‹1 hypothesis
    ğœ®t_R = np.zeros((p,p,T)).astype(complex)
    for t in range(0, T):
        ğšºt =  SCM(ğ—[:, :, t])
        ğœ®t_R[:,:,t] = LR_ğœ®(ğœ®t, R,  Ïƒ2)

    # 5) Compute ratio
    ğ› = np.zeros(p)
    â„“ogâ„’_â„‹0 = 0
    â„“ogâ„’_â„‹1 = 0
    for t in range(0, T):
        â„“ogâ„’_â„‹0 =  â„“ogâ„’_â„‹0 + Gaussian_Log_Likelihood(ğ—[:,:,t], ğ›, ğœ®_R)
        â„“ogâ„’_â„‹1 =  â„“ogâ„’_â„‹1 + Gaussian_Log_Likelihood(ğ—[:,:,t], ğ›, ğœ®t_R[:,:,t])

    if scale=='log':
        return np.real(â„“ogâ„’_â„‹1 - â„“ogâ„’_â„‹0)
    else:
        return np.exp(np.real(â„“ogâ„’_â„‹1 - â„“ogâ„’_â„‹0))


def LR_Plug_in_CM_equality_test(ğ—, args):
    """ Gaussian Plug-in Low-Rank covariance matrix equality GLRT
        ------------------------------------------------------------
        Inputs:
        --------
            * ğ— = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples
                * T = Number of groups of samples
            * args = (R, Ïƒ2, scale) with
                * R = rank (put 0 for adaptive estimation)
                * Ïƒ2 = noise level (put 0 for adaptive estimation)
                * scale = 'linear' or 'log'
        Outputs:
        ---------
            * the plug-in GLRT """

    # 1) Useful variables
    (p, N, T) = ğ—.shape
    R, Ïƒ2, scale = args

    # 2) Estimate R and Ïƒ2 if needed
    if not R:
        R = Rank_estimation(ğ—.reshape((p, N*T)))
    if not Ïƒ2:
        Ïƒ2 = Ïƒ2_estimaton(ğ—.reshape((p,N*T)), R)

    S = SCM(ğ—.reshape((p, N*T)))
    S = LR_ğœ®(S, R,  Ïƒ2)
    logNumerator = N * T * np.log(np.abs(np.linalg.det(S)))
    logDenominator = 0
    for t in range(0, T):
        St = SCM(ğ—[:, :, t])
        St = LR_ğœ®(St, R,  Ïƒ2)
        logDenominator = logDenominator + N * np.log(np.abs(np.linalg.det(St)))
    

    if scale=='log':
        return np.real(logNumerator - logDenominator)
    else:
        return np.exp(np.real(logNumerator - logDenominator))

# ----------------------------------------------------------------------------
# 2) Low-Rank SIRV CM equality statistic
# ----------------------------------------------------------------------------

def tyler_estimator_covariance_low_rank(ğ—, R, Ïƒ2, tol=0.001, iter_max=20):
    """ A function that computes the Tyler Fixed Point Estimator for covariance matrix estimation
        Inputs:
            * ğ— = a matrix of size p*N with each observation along column dimension
            * tol = tolerance for convergence of estimator
            * R = Rank
            * Ïƒ2 = noise level
            * iter_max = number of maximum iterations
        Outputs:
            * ğšº = the estimate
            * Î´ = the final distance between two iterations
            * iteration = number of iterations til convergence """

    # Initialisation
    (p,N) = ğ—.shape
    Î´ = np.inf # Distance between two iterations
    ğšº = SCM(ğ—) # Initialise estimate to identity
    iteration = 0

    # Recursive algorithm
    while (Î´>tol) and (iteration<iter_max):
        
        # Computing expression of Tyler estimator (with matrix multiplication)
        Ï„ = np.diagonal(ğ—.conj().T@np.linalg.inv(ğšº)@ğ—)
        ğ—_bis = ğ— / np.sqrt(Ï„)
        ğšº_new = (p/N) * ğ—_bis@ğ—_bis.conj().T

        # Imposing low rank structure
        ğšº_new = LR_ğœ®(ğšº_new, R, Ïƒ2)

        # Condition for stopping
        Î´ = np.linalg.norm(ğšº_new - ğšº, 'fro') / np.linalg.norm(ğšº, 'fro')
        iteration = iteration + 1

        # Updating ğšº
        ğšº = ğšº_new

    return (ğšº, Î´, iteration)

def tyler_estimator_covariance_matandtext_low_rank(ğ—, R, Ïƒ2, tol, iter_max):
    """ A function that computes the Modified Tyler Fixed Point Estimator Low Rank for 
    covariance matrix estimation under problem MatAndText.
        Inputs:
            * ğ— = a matrix of size p*N*T with each saptial observation along column dimension and time
                observation along third dimension.
            * R = Rank
            * Ïƒ2 = noise lvel
            * tol = tolerance for convergence of estimator
            * iter_max = number of maximum iterations
        Outputs:
            * ğšº = the estimate
            * Î´ = the final distance between two iterations
            * iteration = number of iterations til convergence """
    (p, N, T) = ğ—.shape
    Î´ = np.inf # Distance between two iterations
    ğšº = SCM(ğ—.reshape((p,T*N))) # Initialise estimate to SCM
    iteration = 0

    # Recursive algorithm
    while (Î´>tol) and iteration < iter_max:

        # Compute the textures for each pixel using all the dates available
        Ï„ = 0
        iğšº = np.linalg.inv(ğšº)
        for t in range(0, T):
            Ï„ = Ï„ + np.diagonal(ğ—[:,:,t].conj().T@iğšº@ğ—[:,:,t])

        # Computing expression of the estimator
        ğšº_new = 0
        for t in range(0, T):
            ğ—_bis = ğ—[:,:,t] / np.sqrt(Ï„)
            ğšº_new = ğšº_new + (p/N) * ğ—_bis@ğ—_bis.conj().T

        # Imposing low rank structure
        ğšº_new = LR_ğœ®(ğšº_new, R, Ïƒ2)

        # Condition for stopping
        Î´ = np.linalg.norm(ğšº_new - ğšº, 'fro') / np.linalg.norm(ğšº, 'fro')

        # Updating ğšº
        ğšº = ğšº_new
        iteration = iteration + 1


    return (ğšº, Î´, iteration)

def scale_and_shape_equality_robust_statistic_low_rank(ğ—, args):
    """ Low-Rank GLRT test for testing a change in the scale or/and shape of 
        a deterministic SIRV model.
        Inputs:
            * ğ— = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * args = (tol, iter_max, R, Ïƒ2, scale)
                * tol = tol for tyler estimation
                * iter_max = maximum number of iterations for tyler estimation
                * R = rank (put 0 for adaptive estimation)
                * Ïƒ2 assumed known (boolean) = if true we estimate Ïƒ2 at the beginning, 
                else it is taken as the mean of p-R lowest eigenvalues when needed.
                * scale = 'linear' or 'log'
        Outputs:
            * the statistic given the observations in input"""


    tol, iter_max, R, Ïƒ2, scale = args
    (p, N, T) = ğ—.shape

    # Estimate R and Ïƒ2 if needed
    if not R:
        R = Rank_estimation(ğ—.reshape((p, N*T)))
    if Ïƒ2:
        Ïƒ2 = Ïƒ2_estimaton(ğ—.reshape((p,N*T)), R)
    else:
        Ïƒ2 = None

    # Estimating ğšº_0 using all the observations
    (ğšº_0, Î´, niter) = tyler_estimator_covariance_matandtext_low_rank(ğ—, R, Ïƒ2, tol, iter_max)
    iğšº_0 = np.linalg.inv(ğšº_0)

    # Some initialisation
    log_numerator_determinant_terms = T*N*np.log(np.abs(np.linalg.det(ğšº_0)))
    log_denominator_determinant_terms = 0
    ğ›•_0 = 0
    logğ›•_t = 0

    # Iterating on each date to compute the needed terms
    for t in range(0,T):
        # Estimating ğšº_t
        (ğšº_t, Î´, iteration) = tyler_estimator_covariance_low_rank(ğ—[:,:,t], R, Ïƒ2, tol, iter_max)

        # Computing determinant add adding it to log_denominator_determinant_terms
        log_denominator_determinant_terms = log_denominator_determinant_terms + \
                                            N*np.log(np.abs(np.linalg.det(ğšº_t)))

        # Computing texture estimation
        ğ›•_0 =  ğ›•_0 + np.diagonal(ğ—[:,:,t].conj().T@iğšº_0@ğ—[:,:,t]) / T
        logğ›•_t = logğ›•_t + np.log(np.diagonal(ğ—[:,:,t].conj().T@np.linalg.inv(ğšº_t)@ğ—[:,:,t]))

    # Computing quadratic terms
    log_numerator_quadtratic_terms = T*p*np.sum(np.log(ğ›•_0))
    log_denominator_quadtratic_terms = p*np.sum(logğ›•_t)

    # Final expression of the statistic
    if scale=='linear':
        Î» = np.exp(np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms))
    else:
        Î» = np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms)

    return Î»