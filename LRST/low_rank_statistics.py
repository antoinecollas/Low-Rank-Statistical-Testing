##############################################################################
# Functions to compute Low-Rank statistics
# Authored by Ammar Mian, 10/01/2019
# e-mail: ammar.mian@centralesupelec.fr
##############################################################################
# Copyright 2018 @CentraleSupelec
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
##############################################################################
import sys
import numpy as np
import scipy as sp
from sklearn.decomposition._pca import _assess_dimension_

import warnings

from LRST.generic_functions import *


# ----------------------------------------------------------------------------
# 1) Low-Rank Gaussian CM equality statistic
# ----------------------------------------------------------------------------

def Gaussian_Log_Likelihood(ğ—, ğ›, ğšº):
    """ Gaussian Log-Likelihood of i.i.d observations
        ----------------------------------------------
        Inputs:
        --------
            * ğ— = a (p, N) numpy array with:
                * p = dimension of vectors
                * N = number of Samples
            * ğ› = mean (array of dimension p)
            * ğšº = covariance matrix of dimension (p,p)
        Outputs:
        ---------
            * the Log-Likelihood """
    
    (p, N) = ğ—.shape
    ğ—_centered = ğ— - np.tile(ğ›.reshape((p,1)), (1,N))
    ğ’ = SCM(ğ—_centered)
    â„“ogâ„’ = - N*p*np.log(np.pi) - N*np.log(np.abs(np.linalg.det(ğšº))) - \
            np.trace(ğ’@np.linalg.inv(ğšº))
    return â„“ogâ„’

def LR_ğœ®(ğšº, R,  Ïƒ2):
    """ Low-Rank operator on ğšº
        ----------------------------------------------
        Inputs:
        --------
            * ğšº = covariance matrix of dimension (p,p)
            * R = rank
            * Ïƒ2 = noise level
        Outputs:
        ---------
            * the LR regularised matrix """

    (p,p) = ğšº.shape
    if R==p:
        return ğšº

    u, s, vh = np.linalg.svd(ğšº)
    if Ïƒ2 is None:
        Ïƒ2 = np.mean(s[R:])
    s_signal = np.max([s[:R],Ïƒ2*np.ones((R,))], axis=0)
    s_noise = Ïƒ2 * np.ones((p-R,))
    s = np.diag(np.hstack([s_signal, s_noise]))
    ğšº_R = u @ s @ u.conj().T

    return ğšº_R


def information_criterion(s):
    """ Implements information criterion of AIC/BIC methods
    ----------------------------------------------
    Inputs:
    --------
        * s = (p) numpy array of eigenvalues
    Outputs:
    ---------
        * the information criterion """
    p = len(s)
    k = np.arange(1, p)
    numerator = np.empty(len(k))
    for idx in k:
        power = 1/(p-idx)
        temp = np.prod(s[idx:])**power
        numerator[idx-1] = temp
    denominator = np.cumsum(s[1:][::-1])[::-1]
    ic = -np.log((p-k)*numerator/denominator)
    return ic

def AIC_criterion(s, n):
    """ AIC criterion for order selection
    ----------------------------------------------
    Inputs:
    --------
        * s = a (p) numpy array of eigenvalues of SCM
        * n = number of samples used for computing the SCM
    Outputs:
    ---------
        * the criterion """
    p = len(s)
    k = np.arange(1,p)
    ic = information_criterion(s)
    criterion = n*(p-k)*ic+k*(2*p-k)
    return criterion
    
def AICc_criterion(s, n):
    """ AICc criterion for order selection
    ----------------------------------------------
    Inputs:
    --------
        * s = a (p) numpy array of eigenvalues of SCM
        * n = number of samples used for computing the SCM
    Outputs:
    ---------
        * the criterion """
    p = len(s)
    k = np.arange(1,p)
    ic = information_criterion(s)
    nb_param = k*(2*p-k) + 1
    criterion = n*(p-k)*ic + nb_param + (nb_param**2+nb_param)/((n*p)-nb_param-1)
    return criterion

def BIC_criterion(s, n):
    """ BIC criterion for order selection
    ----------------------------------------------
    Inputs:
    --------
        * s = a (p) numpy array of eigenvalues of SCM
        * n = number of samples used for computing the SCM
    Outputs:
    ---------
        * the criterion """
    p = len(s)
    k = np.arange(1,p)
    ic = information_criterion(s)
    criterion = n*(p-k)*ic+k*(2*p-k)*0.5*np.log(n)
    return criterion

def BIC_minka_criterion(s, n):
    """ BIC criterion for order selection from Minka 2000 "Automatic choice of dimensionality for PCA"
    ----------------------------------------------
    Inputs:
    --------
        * s = a (p) numpy array of eigenvalues of SCM
        * n = number of samples used for computing the SCM
    Outputs:
    ---------
        * the criterion """
    p = len(s)
    k = np.arange(1, p)
    criterion = np.empty(p-1)
    for idx in k:
        m = p*(p-1)/2-(p-idx)*(p-idx-1)/2
        temp0 = (n/2) * np.sum(np.log(s[:idx]))
        temp1 = (n*(p-idx)/2) * np.log(np.sum(s[idx:])/(p-idx))
        criterion[idx-1] = temp0 + temp1 + ((m+idx)/2)*np.log(n)
    return criterion

def Minka_criterion(s, n):
    """ Minka criterion for order selection from Minka 2000 "Automatic choice of dimensionality for PCA"
    ----------------------------------------------
    Inputs:
    --------
        * s = a (p) numpy array of eigenvalues of SCM
        * n = number of samples used for computing the SCM
    Outputs:
    ---------
        * the criterion """
    p = len(s)
    criterion = np.empty(p)
    for rank in range(p):
        criterion[rank] = -_assess_dimension_(s, rank, n, p)
    return criterion

def EDC_criterion(s, n):
    """ EDC criterion for order selection
    ----------------------------------------------
    Inputs:
    --------
        * s = a (p) numpy array of eigenvalues of SCM
        * n = number of samples used for computing the SCM
    Outputs:
    ---------
        * the criterion """
    p = len(s)
    k = np.arange(1,p)
    ic = information_criterion(s)
    criterion = n*(p-k)*ic+k*(2*p-k)*np.sqrt(n*np.log(np.log(n)))
    return criterion

def SCM_rank_criterion(ğ—, method):
    """ Compute the SCM of ğ— and the AIC/BIC criteria for rank estimation
    ----------------------------------------------
    Inputs:
    --------
        * ğ— = a (p, N) numpy array with:
            * p = dimension of vectors
            * N = number of Samples
        * method = 'AIC'/'AICc'/'BIC'/'BIC_Minka'/'Minka'/'EDC' and their thresholded versions
    Outputs:
    ---------
        * the criterion """
    (p, N) = ğ—.shape
    ğšº = SCM(X)
    u, s, vh = np.linalg.svd(ğšº)
    if method == 'AIC':
        criterion = AIC_criterion(s, N)
    elif method == 'AICc':
        criterion = AICc_criterion(s, N)
    elif method == 'BIC':
        criterion = BIC_criterion(s, N)
    elif method == 'BIC_Minka':
        criterion = BIC_minka_criterion(s, N)
    elif method == 'Minka':
        criterion = Minka_criterion(s, N)
    elif method == 'EDC':
        criterion = EDC_criterion(s, N)
    else:
        print('Method', method, 'unknown...')
        sys.exit(1)
    return criterion

def rank_estimation(ğ—, method='AIC'):
    """ order selection using AIC or BIC methods
        ----------------------------------------------
        Inputs:
        --------
            * ğ— = a (p, N) numpy array with:
                * p = dimension of vectors
                * N = number of Samples
        Outputs:
        ---------
            * the Rank """
    method_split = method.split('_')
    if (len(method_split)>=4) or (len(method_split)==2) or ((len(method_split)==3) and (method_split[1]!='thresholded')):
        print('Method', method, 'unknown...')
        sys.exit(1)
    method = method_split[0]
    if len(method_split) == 3:
        threshold = int(method_split[2])
    else:
        threshold = None
    criterion = SCM_rank_criterion(ğ—, method)
    rank = np.argmin(criterion) + 1
    if (threshold is not None) and (rank>threshold):
        rank = threshold
    return rank

def rank_estimation_reshape(ğ—, args):
    """ Estimates rank using AIC rule
    Input:
        * ğ— = a tensor of size (p, ...) with each observation along column dimension
        * args = method to use (either AIC or BIC)
    Outputs:
        * R = rank
    """
    p = ğ—.shape[0]
    ğ— = ğ—.reshape((p, -1))
    R = rank_estimation(ğ—, method=args)
    return R

def eigenvalues_SCM(ğ—, args):
    """ compute SCM and its eigenvalues
    Input:
        * ğ— = a tensor of size (p, ...) with each observation along column dimension
        * args = Nothing
    Outputs:
        * eigenvalues
    """
    p = ğ—.shape[0]
    ğ— = ğ—.reshape((p, -1))
    ğšº = SCM(ğ—)
    s = np.linalg.eigvals(ğšº)
    s = np.sort(s)[::-1]
    return s

def Ïƒ2_estimation(ğ—, R):
    """ Estimate Ïƒ2 locally using samples
        ----------------------------------------------
        Inputs:
        --------
            * ğ— = a (p, N) numpy array with:
                * p = dimension of vectors
                * N = number of Samples
            * R = rank
        Outputs:
        ---------
            * Ïƒ2 """

    (p, N) = ğ—.shape
    ğšº = SCM(ğ—)
    u, s, vh = np.linalg.svd(ğšº)
    if R<p:
        Ïƒ2 = s[R:].mean()
    else:
        Ïƒ2 = 0
    return Ïƒ2


def LR_CM_equality_test(ğ—, args):
    """ Gaussian Low-Rank covariance matrix equality GLRT
        --------------------------------------------------
        Inputs:
        --------
            * ğ— = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples
                * T = Number of groups of samples
            * args = (R, Ïƒ2, scale) with
                * R = rank (put AIC or BIC for adaptive estimation)
                * Ïƒ2 assumed known (boolean) = if true we estimate Ïƒ2 at the beginning, 
                else it is taken as the mean of p-R lowest eigenvalues when needed.
                * scale = 'linear' or 'log'
        Outputs:
        ---------
            * the GLRT """

    # 1) Useful variables
    (p, N, T) = ğ—.shape
    R, Ïƒ2, scale = args

    # 2) Estimate R and Ïƒ2 if needed
    if not (isinstance(R, int)) :
        R = rank_estimation(ğ—.reshape((p, N*T)), R)
    if Ïƒ2:
        Ïƒ2 = Ïƒ2_estimation(ğ—.reshape((p,N*T)), R)
    else:
        Ïƒ2 = None

    # 3) Estimate ğšº_R under â„‹0 hypothesis
    ğšº = SCM(X.reshape((p,N*T)))
    ğœ®_R = LR_ğœ®(ğšº, R, Ïƒ2)

    # 4) Estimate ğšºt_R under â„‹1 hypothesis
    ğœ®t_R = np.zeros((p,p,T)).astype(complex)
    for t in range(0, T):
        ğšºt =  SCM(ğ—[:, :, t])
        ğœ®t_R[:,:,t] = LR_ğœ®(ğœ®t, R,  Ïƒ2)

    # 5) Compute ratio
    ğ› = np.zeros(p)
    â„“ogâ„’_â„‹0 = 0
    â„“ogâ„’_â„‹1 = 0
    for t in range(0, T):
        â„“ogâ„’_â„‹0 =  â„“ogâ„’_â„‹0 + Gaussian_Log_Likelihood(ğ—[:,:,t], ğ›, ğœ®_R)
        â„“ogâ„’_â„‹1 =  â„“ogâ„’_â„‹1 + Gaussian_Log_Likelihood(ğ—[:,:,t], ğ›, ğœ®t_R[:,:,t])

    if scale=='log':
        return np.real(â„“ogâ„’_â„‹1 - â„“ogâ„’_â„‹0)
    else:
        return np.exp(np.real(â„“ogâ„’_â„‹1 - â„“ogâ„’_â„‹0))


def LR_Plug_in_CM_equality_test(ğ—, args):
    """ Gaussian Plug-in Low-Rank covariance matrix equality GLRT
        ------------------------------------------------------------
        Inputs:
        --------
            * ğ— = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples
                * T = Number of groups of samples
            * args = (R, Ïƒ2, scale) with
                * R = rank (put 0 for adaptive estimation)
                * Ïƒ2 assumed known (boolean) = if true we estimate Ïƒ2 at the beginning, 
                else it is taken as the mean of p-R lowest eigenvalues when needed.
                * scale = 'linear' or 'log'
        Outputs:
        ---------
            * the plug-in GLRT """

    # 1) Useful variables
    (p, N, T) = ğ—.shape
    R, Ïƒ2, scale = args

    # 2) Estimate R and Ïƒ2 if needed
    if not (isinstance(R, int)) :
        R = rank_estimation(ğ—.reshape((p, N*T)), R)
    if Ïƒ2:
        Ïƒ2 = Ïƒ2_estimation(ğ—.reshape((p,N*T)), R)
    else:
        Ïƒ2 = None

    S = SCM(ğ—.reshape((p, N*T)))
    S = LR_ğœ®(S, R,  Ïƒ2)
    logNumerator = N * T * np.log(np.abs(np.linalg.det(S)))
    logDenominator = 0
    for t in range(0, T):
        St = SCM(ğ—[:, :, t])
        St = LR_ğœ®(St, R,  Ïƒ2)
        logDenominator = logDenominator + N * np.log(np.abs(np.linalg.det(St)))
    

    if scale=='log':
        return np.real(logNumerator - logDenominator)
    else:
        return np.exp(np.real(logNumerator - logDenominator))

# ----------------------------------------------------------------------------
# 2) Low-Rank SIRV CM equality statistic
# ----------------------------------------------------------------------------

def tyler_estimator_covariance_low_rank(ğ—, R, Ïƒ2, tol=0.001, iter_max=20):
    """ A function that computes the Tyler Fixed Point Estimator for covariance matrix estimation
        Inputs:
            * ğ— = a matrix of size p*N with each observation along column dimension
            * tol = tolerance for convergence of estimator
            * R = Rank
            * Ïƒ2 = noise level
            * iter_max = number of maximum iterations
        Outputs:
            * ğšº = the estimate
            * Î´ = the final distance between two iterations
            * iteration = number of iterations til convergence """

    # Initialisation
    (p,N) = ğ—.shape
    Î´ = np.inf # Distance between two iterations
    ğšº = SCM(ğ—) # Initialise estimate to identity
    iteration = 0

    # Recursive algorithm
    while (Î´>tol) and (iteration<iter_max):
        
        # Computing expression of Tyler estimator (with matrix multiplication)
        Ï„ = np.diagonal(ğ—.conj().T@np.linalg.inv(ğšº)@ğ—)
        ğ—_bis = ğ— / np.sqrt(Ï„)
        ğšº_new = (p/N) * ğ—_bis@ğ—_bis.conj().T

        # Imposing low rank structure
        ğšº_new = LR_ğœ®(ğšº_new, R, Ïƒ2)

        # Normalisation
        ğšº_new = ğšº_new/np.trace(ğšº_new)

        # Condition for stopping
        Î´ = np.linalg.norm(ğšº_new - ğšº, 'fro') / np.linalg.norm(ğšº, 'fro')
        iteration = iteration + 1

        # Updating ğšº
        ğšº = ğšº_new

    return (ğšº, Î´, iteration)

def tyler_estimator_covariance_matandtext_low_rank(ğ—, R, Ïƒ2, tol, iter_max):
    """ A function that computes the Modified Tyler Fixed Point Estimator Low Rank for 
    covariance matrix estimation under problem MatAndText.
        Inputs:
            * ğ— = a matrix of size p*N*T with each saptial observation along column dimension and time
                observation along third dimension.
            * R = Rank
            * Ïƒ2 = noise lvel
            * tol = tolerance for convergence of estimator
            * iter_max = number of maximum iterations
        Outputs:
            * ğšº = the estimate
            * Î´ = the final distance between two iterations
            * iteration = number of iterations til convergence """
    (p, N, T) = ğ—.shape
    Î´ = np.inf # Distance between two iterations
    ğšº = SCM(ğ—.reshape((p,T*N))) # Initialise estimate to SCM
    iteration = 0

    # Recursive algorithm
    while (Î´>tol) and iteration < iter_max:

        # Compute the textures for each pixel using all the dates available
        Ï„ = 0
        iğšº = np.linalg.inv(ğšº)
        for t in range(0, T):
            Ï„ = Ï„ + np.diagonal(ğ—[:,:,t].conj().T@iğšº@ğ—[:,:,t])

        # Computing expression of the estimator
        ğšº_new = 0
        for t in range(0, T):
            ğ—_bis = ğ—[:,:,t] / np.sqrt(Ï„)
            ğšº_new = ğšº_new + (p/N) * ğ—_bis@ğ—_bis.conj().T

        # Imposing low rank structure
        ğšº_new = LR_ğœ®(ğšº_new, R, Ïƒ2)

        # Normalisation
        ğšº_new = ğšº_new/np.trace(ğšº_new)

        # Condition for stopping
        Î´ = np.linalg.norm(ğšº_new - ğšº, 'fro') / np.linalg.norm(ğšº, 'fro')

        # Updating ğšº
        ğšº = ğšº_new
        iteration = iteration + 1


    return (ğšº, Î´, iteration)

def scale_and_shape_equality_robust_statistic_low_rank(ğ—, args):
    """ Low-Rank GLRT test for testing a change in the scale or/and shape of 
        a deterministic SIRV model.
        Inputs:
            * ğ— = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * args = (tol, iter_max, R, Ïƒ2, scale)
                * tol = tol for tyler estimation
                * iter_max = maximum number of iterations for tyler estimation
                * R = rank (put AIC or BIC for adaptive estimation)
                * Ïƒ2 assumed known (boolean) = if true we estimate Ïƒ2 at the beginning, 
                else it is taken as the mean of p-R lowest eigenvalues when needed.
                * scale = 'linear' or 'log'
        Outputs:
            * the statistic given the observations in input"""


    tol, iter_max, R, Ïƒ2, scale = args
    (p, N, T) = ğ—.shape

    # Estimate R and Ïƒ2 if needed
    if not (isinstance(R, int)) :
        R = rank_estimation(ğ—.reshape((p, N*T)), R)
    if Ïƒ2:
        Ïƒ2 = Ïƒ2_estimation(ğ—.reshape((p,N*T)), R)
    else:
        Ïƒ2 = None

    # Estimating ğšº_0 using all the observations
    (ğšº_0, Î´, niter) = tyler_estimator_covariance_matandtext_low_rank(ğ—, R, Ïƒ2, tol, iter_max)
    iğšº_0 = np.linalg.inv(ğšº_0)

    # Some initialisation
    log_numerator_determinant_terms = T*N*np.log(np.abs(np.linalg.det(ğšº_0)))
    log_denominator_determinant_terms = 0
    ğ›•_0 = 0
    logğ›•_t = 0

    # Iterating on each date to compute the needed terms
    for t in range(0,T):
        # Estimating ğšº_t
        (ğšº_t, Î´, iteration) = tyler_estimator_covariance_low_rank(ğ—[:,:,t], R, Ïƒ2, tol, iter_max)

        # Computing determinant add adding it to log_denominator_determinant_terms
        log_denominator_determinant_terms = log_denominator_determinant_terms + \
                                            N*np.log(np.abs(np.linalg.det(ğšº_t)))

        # Computing texture estimation
        ğ›•_0 =  ğ›•_0 + np.diagonal(ğ—[:,:,t].conj().T@iğšº_0@ğ—[:,:,t]) / T
        logğ›•_t = logğ›•_t + np.log(np.diagonal(ğ—[:,:,t].conj().T@np.linalg.inv(ğšº_t)@ğ—[:,:,t]))

    # Computing quadratic terms
    log_numerator_quadtratic_terms = T*p*np.sum(np.log(ğ›•_0))
    log_denominator_quadtratic_terms = p*np.sum(logğ›•_t)

    # Final expression of the statistic
    if scale=='linear':
        Î» = np.exp(np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms))
    else:
        Î» = np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms)

    return Î»